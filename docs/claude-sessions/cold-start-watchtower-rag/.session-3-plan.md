# Session 03 Implementation Plan — Watchtower Runtime and Loop-Back

## Overview

This session implements the notify-based Watchtower filesystem watcher, the shared ingest pipeline used by both automatic and manual triggers, the source-file loop-back that writes published metadata back into originating notes, and the server integration that starts/stops the Watchtower alongside the existing runtime.

## Key Design Decisions

### D1: No `ContentSource` trait in S03 — Inline `LocalFileSource` logic in Watchtower

**Decision:** The S02 handoff proposed a `ContentSource` trait + `LocalFileSource` struct under `core/source/`. After review, this session's deliverables list does *not* include `core/source/` — the session instructions explicitly list `automation/watchtower.rs`, not `source/`. The AD-1 architecture decision describes the trait living in Foundation layer, but the session instructions focus on the automation layer.

**Approach:** Build the filesystem scanning, reading, hashing, and loop-back logic directly into `automation/watchtower.rs` (and a `watchtower/` module directory if needed for size). Keep functions stateless and modular so they can be extracted into a `ContentSource` trait in a future session. This avoids creating a trait with only one implementation before we know the Google Drive adapter's needs.

**Reversal:** After re-reading the S02 handoff more carefully, it explicitly says S03 must deliver the `ContentSource` trait and `LocalFileSource`. And the session tasks reference "reuse the same ingest pipeline" which implies a shared abstraction. However, the *session instructions* provided to this session say nothing about `source/mod.rs` or `local_fs.rs` — the deliverables are exclusively in `automation/`, `storage/`, `server/`, and docs. I'll follow the actual session deliverables list.

**Final approach:** Implement scanning, reading, hashing, and loop-back as free functions within the `watchtower` module. These form the "shared ingest pipeline" that both filesystem events and POST /api/ingest use. This satisfies the session's stated exit criteria without introducing modules outside the deliverables list.

### D2: Module directory pattern for `automation/watchtower/`

The watchtower module will likely exceed 500 lines (watcher loop + scanning + loop-back + tests). Following the project convention and the `storage/watchtower/` precedent from S02:

```
automation/watchtower/
  mod.rs       — WatchtowerLoop struct, run(), shared ingest pipeline functions
  loopback.rs  — Loop-back metadata writing (idempotent YAML front-matter)
  tests.rs     — All watchtower tests (tempdir-based)
```

### D3: Debouncing strategy — `notify-debouncer-full` not `notify-debouncer-mini`

Use `notify-debouncer-full` (v0.4) which provides full event deduplication with file ID tracking. This handles the rename-then-write pattern common in editors like Obsidian (which writes to a temp file, then renames). The debouncer accumulates events for a configurable window (2 seconds) before emitting.

### D4: Self-event prevention — Cooldown set + content hash double-check

When loop-back writes metadata to a file, two mechanisms prevent infinite re-ingestion:
1. **Cooldown set**: A `HashMap<PathBuf, Instant>` tracks recently-written paths with 5-second expiry. Events for paths in cooldown are silently dropped.
2. **Content hash**: Even if cooldown expires, the content_hash check in `upsert_content_node` returns `Skipped` for unchanged content, preventing duplicate processing.

### D5: Shared ingest pipeline — `ingest_file()` function

Create a `pub async fn ingest_file(pool, source_id, base_path, relative_path, force)` function in `automation/watchtower/mod.rs` that:
1. Reads the file content
2. Parses optional YAML front-matter for title/tags
3. Computes SHA-256 hash
4. Calls `storage::watchtower::upsert_content_node()`

Both the Watchtower's filesystem event handler and POST /api/ingest's `file_hints` path call this same function. This guarantees behavioral identity per exit criteria.

### D6: Server integration — Optional watchtower cancellation token in AppState

Add an `Option<CancellationToken>` for the watchtower to `AppState`. When the server starts and `content_sources.sources` has entries with `watch = true`, spawn the watchtower loop. When the server shuts down, cancel the token. This follows the existing `runtimes` pattern but is simpler since the watchtower is a singleton background service, not per-account.

### D7: Loop-back format — YAML front-matter with `tuitbot` key

Per AD-1, published metadata is written as YAML front-matter:
```yaml
---
tuitbot:
  - tweet_id: "1234567890"
    url: "https://x.com/user/status/1234567890"
    published_at: "2026-02-28T14:30:00Z"
    type: "tweet"
---
```

Idempotency: Before writing, parse existing front-matter and check if `tweet_id` already exists in the `tuitbot` array. If so, skip.

### D8: `serde_yaml` for front-matter parsing

Use `serde_yaml` 0.9 for parsing and serializing YAML front-matter blocks. This is a well-maintained crate already planned in the S02 handoff's dependency list.

---

## Files to Create

### 1. `crates/tuitbot-core/src/automation/watchtower/mod.rs`

**Purpose:** Watchtower service — notify-driven filesystem watcher + shared ingest pipeline.

**Contents:**
- `WatchtowerLoop` struct holding `DbPool`, source config, debounce settings, cooldown map
- `pub async fn run(&self, cancel: CancellationToken)` — main loop:
  1. Register source contexts in DB for each configured `ContentSourceEntry`
  2. Perform initial full scan of configured directories
  3. Start `notify` watcher with debouncer
  4. Select loop: listen for debounced events OR fallback interval scan OR cancellation
  5. On event: filter by file pattern → check cooldown → call `ingest_file()`
  6. On cancellation: flush pending, drop watcher
- `pub async fn ingest_file(pool, source_id, base_path, relative_path, force) -> Result<UpsertResult, WatchtowerError>` — shared pipeline:
  1. Read file content from disk
  2. Parse YAML front-matter (extract title, tags)
  3. Compute SHA-256 content hash
  4. Call `storage::watchtower::upsert_content_node()`
- `pub async fn ingest_files(pool, source_id, base_path, paths, force) -> IngestSummary` — batch wrapper
- `fn matches_patterns(path, patterns) -> bool` — glob pattern matching for `.md`/`.txt` filtering
- `fn parse_front_matter(content: &str) -> (Option<FrontMatter>, &str)` — extract YAML between `---` delimiters
- `pub struct WatchtowerError` — thiserror enum with `Io`, `Storage`, `Notify` variants
- `pub struct IngestSummary { pub ingested: u32, pub skipped: u32, pub errors: Vec<String> }`
- Re-export key types

**Estimated size:** ~350 lines (well under 500 with logic split into loopback.rs)

### 2. `crates/tuitbot-core/src/automation/watchtower/loopback.rs`

**Purpose:** Loop-back metadata writing — idempotent YAML front-matter operations.

**Contents:**
- `pub struct LoopBackMetadata` — tweet_id, url, published_at, content_type
- `pub fn write_metadata_to_file(path: &Path, metadata: &LoopBackMetadata) -> Result<(), io::Error>` — idempotent write:
  1. Read current file content
  2. Parse existing front-matter
  3. Check if `tweet_id` already in `tuitbot` array → skip if so
  4. Add new entry to `tuitbot` array
  5. Serialize updated front-matter + body back to file
- `pub fn parse_tuitbot_metadata(content: &str) -> Vec<LoopBackMetadata>` — parse existing entries
- `fn has_front_matter(content: &str) -> bool`
- `fn split_front_matter(content: &str) -> (Option<String>, &str)` — split `---`-delimited YAML from body
- `fn merge_front_matter(existing_yaml: Option<&str>, metadata: &LoopBackMetadata) -> String` — produce updated YAML block

**Estimated size:** ~200 lines

### 3. `crates/tuitbot-core/src/automation/watchtower/tests.rs`

**Purpose:** Tempdir-based tests for all watchtower functionality.

**Tests to implement:**

| Test | Description |
|------|-------------|
| `ingest_file_creates_content_node` | Write .md to tempdir → `ingest_file()` → query DB → node exists with correct hash, title, body |
| `ingest_file_with_front_matter` | File with YAML front-matter → `ingest_file()` → title and tags extracted from front-matter |
| `ingest_file_dedup_by_hash` | Ingest same file twice → second returns `Skipped` |
| `ingest_file_updates_on_change` | Ingest → modify file → reingest → returns `Updated` |
| `ingest_file_force_bypasses_hash` | Ingest → reingest with `force=true` → returns `Inserted` or `Updated` |
| `matches_patterns_md_and_txt` | `matches_patterns("note.md", &["*.md", "*.txt"])` → true |
| `matches_patterns_rejects_jpg` | `matches_patterns("photo.jpg", &["*.md", "*.txt"])` → false |
| `matches_patterns_nested_path` | `matches_patterns("sub/dir/note.md", &["*.md"])` → true |
| `parse_front_matter_extracts_yaml` | File with `---\ntitle: Test\n---\nbody` → returns `(Some(fm), "body")` |
| `parse_front_matter_no_yaml` | File without front-matter → returns `(None, full_content)` |
| `loopback_write_new_file` | File with no front-matter → `write_metadata_to_file()` → file has tuitbot YAML |
| `loopback_write_existing_frontmatter` | File with existing YAML → `write_metadata_to_file()` → tuitbot key added, existing keys preserved |
| `loopback_idempotent` | `write_metadata_to_file()` twice with same tweet_id → only one entry |
| `loopback_multiple_tweets` | Write metadata for tweet A → write for tweet B → tuitbot array has 2 entries |
| `cooldown_prevents_reingest` | Add path to cooldown → check → returns true (blocked) |
| `cooldown_expires` | Add path → advance time past expiry → check → returns false (allowed) |
| `batch_ingest_summary` | Ingest 3 files (2 new, 1 duplicate) → summary shows ingested=2, skipped=1 |
| `watcher_respects_cancellation` | Start watcher loop → cancel token → loop exits within timeout |

**Estimated size:** ~350 lines

### 4. `docs/roadmap/cold-start-watchtower-rag/session-03-handoff.md`

**Purpose:** Session handoff document for S04.

---

## Files to Modify

### 5. `crates/tuitbot-core/src/automation/mod.rs`

**Changes:**
- Add `pub mod watchtower;` to the module list (line ~30, after `thread_loop`)
- Add re-exports: `pub use watchtower::{WatchtowerLoop, WatchtowerError, IngestSummary};`

### 6. `crates/tuitbot-core/Cargo.toml`

**Changes:**
- Add `notify = "7"` to `[dependencies]`
- Add `notify-debouncer-full = "0.4"` to `[dependencies]`
- Add `serde_yaml = "0.9"` to `[dependencies]`
- Add `glob = "0.3"` to `[dependencies]` (for pattern matching)

Note: `sha2` is already a dependency. `tempfile` is already a dev-dependency.

### 7. `crates/tuitbot-server/src/state.rs`

**Changes:**
- Add `use tokio_util::sync::CancellationToken;` import
- Add field to `AppState`:
  ```rust
  /// Cancellation token for the Watchtower filesystem watcher (None if not configured).
  pub watchtower_cancel: Option<CancellationToken>,
  ```

### 8. `crates/tuitbot-server/src/main.rs`

**Changes:**
- After creating `AppState`, check if `content_sources.sources` has any `watch: true` entries
- If so, spawn the watchtower loop as a background task:
  ```rust
  let watchtower_cancel = if let Some(config) = &loaded_config {
      let watch_sources: Vec<_> = config.content_sources.sources.iter()
          .filter(|s| s.watch && s.path.is_some())
          .collect();
      if !watch_sources.is_empty() {
          let cancel = CancellationToken::new();
          let watchtower = WatchtowerLoop::new(pool.clone(), config.content_sources.clone());
          let cancel_clone = cancel.clone();
          tokio::spawn(async move {
              watchtower.run(cancel_clone).await;
          });
          tracing::info!(sources = watch_sources.len(), "Watchtower started");
          Some(cancel)
      } else {
          None
      }
  } else {
      None
  };
  ```
- Add `watchtower_cancel` to `AppState` construction
- Before server shutdown, cancel the watchtower if running

### 9. `crates/tuitbot-server/src/routes/ingest.rs`

**Changes:**
- Wire `file_hints` to the shared `ingest_file()` pipeline:
  ```rust
  // Process file_hints by scanning specified paths through the shared pipeline.
  if !body.file_hints.is_empty() {
      // Find the first local_fs source context, or create one
      let source_ctx = find_or_create_fs_source(&state.db, &state.config_path).await?;
      for hint_path in &body.file_hints {
          match watchtower::ingest_file(&state.db, source_ctx.id, &source_ctx.base_path, hint_path, body.force).await {
              Ok(UpsertResult::Inserted | UpsertResult::Updated) => ingested += 1,
              Ok(UpsertResult::Skipped) => skipped += 1,
              Err(e) => errors.push(format!("{hint_path}: {e}")),
          }
      }
  }
  ```
- Import `WatchtowerLoop` or the shared `ingest_file` function from core

### 10. `crates/tuitbot-core/src/storage/watchtower/mod.rs`

**Changes:**
- Add helper function to find source context by path:
  ```rust
  pub async fn find_source_by_path(pool: &DbPool, path: &str) -> Result<Option<SourceContext>, StorageError>
  ```
- Add helper to ensure a local_fs source context exists for a given path (similar to `ensure_manual_source`):
  ```rust
  pub async fn ensure_local_fs_source(pool: &DbPool, path: &str, config_json: &str) -> Result<i64, StorageError>
  ```

---

## Risks and Mitigations

### Risk 1: `notify` crate cross-platform behavior (FSEvents on macOS vs inotify on Linux)

**Impact:** Duplicate or missed events
**Mitigation:**
- Use `notify-debouncer-full` which handles cross-platform deduplication
- 2-second debounce window is generous enough for editor save patterns
- Fallback interval scan (5 min) catches anything the watcher misses
- Content hash check prevents duplicate processing regardless

### Risk 2: Loop-back writes triggering re-ingestion (infinite loop)

**Impact:** Runaway ingestion cycles, potential CPU spike
**Mitigation:**
- Cooldown set with 5-second expiry for recently-written paths
- Content hash check as second safety net
- Test specifically for this scenario (`cooldown_prevents_reingest`)

### Risk 3: YAML front-matter parsing corrupting user notes

**Impact:** Data loss in user's Obsidian vault
**Mitigation:**
- Use well-tested `serde_yaml` for parsing/serialization
- Read-parse-merge-write pattern preserves all existing content
- Idempotent writes prevent duplicate metadata entries
- `loop_back_enabled` config flag allows users to disable (default: true)
- Test with various front-matter scenarios (none, existing, partial)

### Risk 4: Large vaults causing slow initial scan

**Impact:** Server startup delay
**Mitigation:**
- Initial scan runs in background (spawned task, doesn't block HTTP)
- Content hash skips unchanged files on subsequent scans
- `sync_cursor` tracks last scan time for incremental scanning

### Risk 5: New dependencies (`notify`, `serde_yaml`, `glob`) adding build time

**Impact:** Slower CI builds
**Mitigation:**
- All three are well-maintained, pure-Rust crates with minimal transitive deps
- `notify` is the largest but is a standard choice for filesystem watching in Rust
- No alternative exists that's lighter without sacrificing correctness

### Risk 6: `notify` watcher consuming too many file descriptors on large vaults

**Impact:** OS-level resource exhaustion
**Mitigation:**
- `notify` uses FSEvents on macOS (system-level, doesn't use per-file FDs)
- On Linux, inotify has a per-process watch limit (typically 65536)
- For very large vaults, the fallback interval scan is sufficient without the watcher
- Document the limitation in the handoff

---

## Order of Operations

### Phase 1: Dependencies and storage additions (~15 min)

1. Add `notify`, `notify-debouncer-full`, `serde_yaml`, `glob` to `crates/tuitbot-core/Cargo.toml`
2. Add `find_source_by_path()` and `ensure_local_fs_source()` to `storage/watchtower/mod.rs`
3. Run `cargo check -p tuitbot-core` to verify deps resolve

### Phase 2: Loop-back module (`automation/watchtower/loopback.rs`) (~25 min)

4. Create `automation/watchtower/` directory structure
5. Implement `LoopBackMetadata` struct
6. Implement `split_front_matter()` — parse `---`-delimited YAML
7. Implement `merge_front_matter()` — add tuitbot entry to existing/new YAML
8. Implement `write_metadata_to_file()` — full read-parse-merge-write flow
9. Implement `parse_tuitbot_metadata()` — read existing entries

### Phase 3: Shared ingest pipeline (`automation/watchtower/mod.rs`) (~30 min)

10. Create `WatchtowerError` enum with `Io`, `Storage`, `Notify`, `Config` variants
11. Implement `parse_front_matter()` — extract title/tags from YAML
12. Implement `matches_patterns()` — glob pattern matching
13. Implement `ingest_file()` — read + hash + parse + upsert
14. Implement `IngestSummary` struct
15. Implement `ingest_files()` — batch wrapper calling `ingest_file()` in loop

### Phase 4: Watchtower loop (`automation/watchtower/mod.rs`) (~30 min)

16. Implement `WatchtowerLoop` struct with config, db pool, cooldown map
17. Implement `WatchtowerLoop::new()` constructor
18. Implement `register_sources()` — create/find source_context rows for each configured source
19. Implement `initial_scan()` — walk configured directories, ingest all matching files
20. Implement `run()` — main loop with notify watcher + debouncer + fallback scan + cancellation
21. Implement cooldown set logic (add, check, cleanup expired entries)
22. Add `pub mod watchtower;` and re-exports to `automation/mod.rs`

### Phase 5: Server integration (~20 min)

23. Add `watchtower_cancel: Option<CancellationToken>` to `AppState` in `state.rs`
24. Update `main.rs` to conditionally spawn watchtower loop on startup
25. Update `main.rs` to cancel watchtower on shutdown
26. Wire `file_hints` in `routes/ingest.rs` to call shared `ingest_file()` pipeline
27. Run `cargo check -p tuitbot-server` to verify integration compiles

### Phase 6: Tests (`automation/watchtower/tests.rs`) (~30 min)

28. Write `ingest_file_creates_content_node` — happy path
29. Write `ingest_file_with_front_matter` — title/tag extraction
30. Write `ingest_file_dedup_by_hash` — hash-based dedup
31. Write `matches_patterns_*` tests — pattern matching
32. Write `parse_front_matter_*` tests — YAML extraction
33. Write `loopback_*` tests — idempotent metadata writing
34. Write `cooldown_*` tests — self-event prevention
35. Write `batch_ingest_summary` — batch wrapper
36. Write `watcher_respects_cancellation` — graceful shutdown

### Phase 7: Quality gates and handoff (~15 min)

37. Run `cargo fmt --all && cargo fmt --all --check`
38. Run `RUSTFLAGS="-D warnings" cargo test --workspace`
39. Run `cargo clippy --workspace -- -D warnings`
40. Fix any failures
41. Write `docs/roadmap/cold-start-watchtower-rag/session-03-handoff.md`

---

## Verification Steps

### Unit tests (Phase 6)

All tests use `init_test_db()` + `tempfile::tempdir()`. No network calls, no timing-dependent assertions (except watcher cancellation with generous 5s timeout).

### Integration verification

After all code is written:

```bash
# Format
cargo fmt --all && cargo fmt --all --check

# Tests (all workspace tests must pass, including existing 1567+)
RUSTFLAGS="-D warnings" cargo test --workspace

# Lint
cargo clippy --workspace -- -D warnings
```

### Exit criteria validation

1. **"Changing a watched .md or .txt file enqueues exactly one ingest record after debounce"**
   - Verified by `ingest_file_creates_content_node` test + `watcher_debounces_rapid_writes` if timing permits, otherwise by the hash-based dedup test proving that even without perfect debouncing, duplicate events produce one record.

2. **"Manual and automatic ingest paths share the same persisted state transitions"**
   - Verified by the fact that both paths call the same `ingest_file()` function, which calls the same `storage::watchtower::upsert_content_node()`. The `file_hints` wiring in `routes/ingest.rs` uses the identical code path.

3. **"Re-running loop-back on an already-annotated note does not duplicate metadata"**
   - Verified by `loopback_idempotent` test.

---

## Dependency Graph

```
automation/watchtower/mod.rs
  ├── automation/watchtower/loopback.rs   (loop-back writing)
  ├── storage/watchtower/mod.rs           (DB CRUD — existing)
  ├── config/types.rs                     (ContentSourceEntry — existing)
  ├── notify + notify-debouncer-full      (filesystem events)
  ├── serde_yaml                          (front-matter parsing)
  ├── sha2                                (content hashing — existing dep)
  └── glob                                (pattern matching)

server/main.rs
  ├── automation/watchtower/mod.rs        (WatchtowerLoop)
  └── server/state.rs                     (AppState with watchtower_cancel)

server/routes/ingest.rs
  └── automation/watchtower/mod.rs        (ingest_file — shared pipeline)
```

---

## Open Questions (to resolve during implementation)

1. **Front-matter parsing edge cases:** What if the file has TOML front-matter (`+++` delimiters) instead of YAML (`---`)? For v1, only support YAML (the Obsidian standard). Document this limitation.

2. **Watcher scope:** Should the watcher watch recursively or only the top-level directory? **Answer:** Recursively — Obsidian vaults have nested folder structures. `notify`'s `RecursiveMode::Recursive` handles this.

3. **Initial scan performance:** For a vault with 10,000+ files, should the initial scan be rate-limited? **Answer:** No — the scan runs in a background task and doesn't block HTTP. SHA-256 hashing is CPU-bound but fast (GiB/s on modern hardware). The bottleneck is SQLite writes, which are batched by the connection pool.

4. **Config reload:** If the user changes `content_sources` in config.toml while the server is running, should the watchtower restart? **Answer:** Not in v1. The watchtower reads config at startup. Restarting the server picks up new config. This matches the existing pattern for all other config sections.
