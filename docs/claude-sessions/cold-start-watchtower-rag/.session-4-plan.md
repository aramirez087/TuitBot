# Session 04 Implementation Plan — Winning DNA Classification & Analytics-Weighted Retrieval

## Summary

This session adds the Winning DNA pipeline: classifying historical tweet output into archetypes, computing engagement-weighted success scores, retrieving high-performing "ancestors" as context for new drafts, blending ingested note context from the Watchtower, and running a background seed pre-compute worker. The result is a RAG-enriched draft path where the LLM receives examples of the user's best-performing content patterns alongside their ingested notes.

---

## Files to Create

| # | File | Purpose | Est. Lines |
|---|------|---------|-----------|
| 1 | `crates/tuitbot-core/src/context/winning_dna.rs` | Archetype classification, engagement scoring, ranked ancestor retrieval, cold-start fallback | ~350 |
| 2 | `crates/tuitbot-core/src/context/winning_dna_tests.rs` | Deterministic tests for classification, scoring, ranking, and cold-start | ~250 |
| 3 | `crates/tuitbot-core/src/automation/seed_worker.rs` | Low-priority background worker that pre-computes draft seeds from content nodes | ~180 |
| 4 | `docs/roadmap/cold-start-watchtower-rag/rag-ranking.md` | Scoring rules, thresholds, retrieval algorithm documentation | ~120 |
| 5 | `docs/roadmap/cold-start-watchtower-rag/session-04-handoff.md` | Handoff document for Session 05 | ~100 |

## Files to Modify

| # | File | Change Summary |
|---|------|---------------|
| 6 | `crates/tuitbot-core/src/context/mod.rs` | Add `pub mod winning_dna;` |
| 7 | `crates/tuitbot-core/src/storage/analytics.rs` | Add `update_archetype_vibe()`, `update_engagement_score()`, `get_top_ancestors()`, `get_max_performance_score()` |
| 8 | `crates/tuitbot-core/src/storage/watchtower/mod.rs` | Add `get_pending_content_nodes()`, `mark_node_processed()`, `insert_draft_seed_with_weight()`, `get_seeds_for_context()` |
| 9 | `crates/tuitbot-core/src/workflow/draft.rs` | Integrate RAG context from winning ancestors + content nodes into LLM prompt |
| 10 | `crates/tuitbot-core/src/content/generator.rs` | Add `generate_reply_with_context()` method that accepts optional RAG context block |
| 11 | `crates/tuitbot-core/src/automation/mod.rs` | Add `pub mod seed_worker;` and re-export `SeedWorker` |

---

## Detailed Design

### 1. `context/winning_dna.rs` — Core Classification & Retrieval

**Structs:**

```rust
/// A historically successful tweet classified with its engagement data.
pub struct WinningAncestor {
    pub tweet_id: String,
    pub content_preview: String,     // truncated to 120 chars
    pub content_type: String,        // "reply" or "tweet"
    pub archetype_vibe: String,      // classified archetype/format name
    pub engagement_score: f64,       // 0.0-1.0 normalized
    pub retrieval_weight: f64,       // engagement_score * recency_decay
    pub posted_at: String,           // ISO-8601
}

/// Context block ready for injection into LLM prompts.
pub struct DraftContext {
    pub winning_ancestors: Vec<WinningAncestor>,
    pub content_seeds: Vec<ContentSeedContext>,
    pub prompt_block: String,        // formatted text for LLM injection
}

/// A content seed from an ingested note, for cold-start context.
pub struct ContentSeedContext {
    pub seed_text: String,
    pub source_title: Option<String>,
    pub archetype_suggestion: Option<String>,
    pub engagement_weight: f64,
}
```

**Functions:**

1. **`classify_reply_archetype(content: &str) -> String`** — Deterministic rule-based classifier that maps reply text to one of the 5 `ReplyArchetype` variants by keyword/pattern matching. No LLM call — pure heuristics:
   - Starts with question mark or "what/how/why" → `ask_question`
   - Contains "I've found/experienced/noticed" → `share_experience`
   - Contains "data/stats/study/research shows" → `add_data`
   - Contains "actually/however/but" with disagreement signals → `respectful_disagree`
   - Default → `agree_and_expand`

2. **`classify_tweet_format(content: &str) -> String`** — Similar pattern-based classifier for `TweetFormat`:
   - Contains numbered list items → `list`
   - Contains "most people think" / "everyone says" → `most_people_think_x` / `contrarian_take`
   - Contains "before/after" → `before_after`
   - Ends with "?" → `question`
   - Short actionable statement → `tip`
   - Default → `storytelling`

3. **`compute_engagement_score(performance_score: f64, max_score: f64) -> f64`** — Normalizes a raw performance_score to 0.0-1.0 range:
   ```
   if max_score <= 0.0 { return 0.5; }  // cold-start baseline
   (performance_score / max_score).clamp(0.0, 1.0)
   ```

4. **`compute_retrieval_weight(engagement_score: f64, days_since: f64, half_life: f64) -> f64`** — Exponential decay weighting:
   ```
   engagement_score * (-0.693 * days_since / half_life).exp()
   ```

5. **`retrieve_ancestors(pool, topic_keywords, max_results, half_life_days) -> Vec<WinningAncestor>`** — Main retrieval function:
   - Query `tweet_performance` JOIN `original_tweets` + `reply_performance` JOIN `replies_sent` WHERE `engagement_score IS NOT NULL` and topic/keyword matching
   - Compute `retrieval_weight` for each
   - Sort by `retrieval_weight DESC`, take top K
   - Filter out ancestors with `engagement_score < MIN_ENGAGEMENT_SCORE` (0.1)

6. **`retrieve_cold_start_seeds(pool, max_results) -> Vec<ContentSeedContext>`** — Fallback when no performance data exists:
   - Query `draft_seeds` WHERE status = 'pending' ORDER BY `engagement_weight DESC` LIMIT max_results
   - Join with `content_nodes` for title

7. **`build_draft_context(pool, topic_keywords, max_ancestors, half_life_days) -> DraftContext`** — Orchestrator:
   - Call `retrieve_ancestors()` first
   - If empty, call `retrieve_cold_start_seeds()`
   - Format the `prompt_block` string for LLM injection
   - Cap prompt block at 2000 characters (not tokens — simpler, conservative estimate at ~4 chars/token = ~500 tokens)

**Key design decisions:**

- **Rule-based classification, not LLM-based:** Classifiers use deterministic keyword/pattern matching. This keeps classification free from LLM costs, fully testable, and fast. The heuristics don't need to be perfect — they're used for retrieval weighting, not display.
- **Recency half-life = 14 days:** Content success patterns change. A tweet that performed well 2 weeks ago should still influence but less than yesterday's hit.
- **Min engagement score = 0.1:** Filters out the worst-performing ancestors to avoid teaching the LLM from failures.
- **Max ancestors = 5:** Keeps prompt additions manageable (~500 chars × 5 = ~2500 chars before formatting).
- **Character cap, not token cap:** The `rag_max_tokens` threshold from AD-5 is operationalized as a 2000-character cap on the prompt block, which is conservative (~500 tokens).

### 2. Storage Additions

#### `storage/analytics.rs` additions (~60 lines)

```rust
/// Update the archetype_vibe for a tweet performance record.
pub async fn update_tweet_archetype(pool, tweet_id, archetype_vibe) -> Result<()>

/// Update the archetype_vibe for a reply performance record.
pub async fn update_reply_archetype(pool, reply_id, archetype_vibe) -> Result<()>

/// Update the engagement_score for a tweet performance record.
pub async fn update_tweet_engagement_score(pool, tweet_id, score) -> Result<()>

/// Update the engagement_score for a reply performance record.
pub async fn update_reply_engagement_score(pool, reply_id, score) -> Result<()>

/// Get the maximum performance_score across all tweets and replies.
/// Used to normalize engagement scores.
pub async fn get_max_performance_score(pool) -> Result<f64>

/// Row type for ancestor retrieval.
pub struct AncestorRow { ... }

/// Query scored ancestors matching topic keywords, with engagement_score populated.
/// Returns raw rows for the winning_dna module to compute retrieval_weight.
pub async fn get_scored_ancestors(
    pool, topic_keywords: &[String], limit: u32
) -> Result<Vec<AncestorRow>>
```

**SQL for `get_scored_ancestors`:**
```sql
SELECT 'tweet' as content_type, ot.tweet_id, SUBSTR(ot.content, 1, 120),
       tp.archetype_vibe, tp.engagement_score, tp.performance_score,
       ot.created_at
FROM tweet_performance tp
JOIN original_tweets ot ON ot.tweet_id = tp.tweet_id
WHERE tp.engagement_score IS NOT NULL
  AND tp.engagement_score >= ?  -- min_engagement_score
  AND (ot.topic IN (<<placeholders>>) OR ot.content LIKE '%keyword%')
UNION ALL
SELECT 'reply', rp.reply_id, SUBSTR(rs.reply_content, 1, 120),
       rp.archetype_vibe, rp.engagement_score, rp.performance_score,
       rs.created_at
FROM reply_performance rp
JOIN replies_sent rs ON rs.reply_tweet_id = rp.reply_id
WHERE rp.engagement_score IS NOT NULL
  AND rp.engagement_score >= ?
ORDER BY engagement_score DESC
LIMIT ?
```

**Topic matching strategy:** For v1, use `topic IN (...)` for original tweets (which have an explicit `topic` column) and `content LIKE '%keyword%'` for replies (which don't). The LIKE clause is acceptable for SQLite with small datasets. This avoids adding a full-text search dependency in this session.

#### `storage/watchtower/mod.rs` additions (~80 lines)

```rust
/// Get content nodes with status='pending' that need seed generation.
pub async fn get_pending_content_nodes(pool, limit: u32) -> Result<Vec<ContentNode>>

/// Mark a content node as 'processed' after seed generation.
pub async fn mark_node_processed(pool, node_id: i64) -> Result<()>

/// Insert a draft seed with an explicit engagement weight.
pub async fn insert_draft_seed_with_weight(
    pool, node_id, seed_text, archetype_suggestion, weight
) -> Result<i64>

/// Retrieve draft seeds suitable for cold-start context injection.
/// Returns seeds with their parent node's title, ordered by engagement_weight DESC.
pub async fn get_seeds_for_context(pool, limit: u32) -> Result<Vec<SeedWithContext>>

/// Row type for seeds with context.
pub struct SeedWithContext {
    pub seed_text: String,
    pub source_title: Option<String>,
    pub archetype_suggestion: Option<String>,
    pub engagement_weight: f64,
}
```

### 3. Draft Pipeline Integration

#### `content/generator.rs` changes (~40 lines)

Add a new method to `ContentGenerator`:

```rust
/// Generate a reply with optional RAG context injected into the prompt.
///
/// The `rag_context` block is inserted between the persona/voice section
/// and the rules section of the system prompt. If None, behaves identically
/// to `generate_reply_with_archetype`.
pub async fn generate_reply_with_context(
    &self,
    tweet_text: &str,
    tweet_author: &str,
    mention_product: bool,
    archetype: Option<ReplyArchetype>,
    rag_context: Option<&str>,
) -> Result<GenerationOutput, LlmError>
```

Implementation: Identical to `generate_reply_with_archetype` except the system prompt is assembled with an additional `{rag_context}` section placed after `{persona_section}` and before the `Rules:` block. If `rag_context` is None, the section is empty — zero behavioral change for existing callers.

**No breaking changes:** `generate_reply_with_archetype` is preserved as-is. The new method calls the same retry/truncation logic. The `workflow/draft.rs` module switches to calling `generate_reply_with_context`.

#### `workflow/draft.rs` changes (~35 lines)

Modify the `execute()` function:

1. Before the candidate loop, call `winning_dna::build_draft_context(db, keywords, 5, 14.0)` where `keywords` is derived from the config's `product_keywords + competitor_keywords + industry_topics`.
2. Pass `context.prompt_block.as_deref()` (or `None` if empty) to `gen.generate_reply_with_context()` instead of `gen.generate_reply_with_archetype()`.
3. The `DraftInput` struct gains no new fields — the context is sourced from DB, not from caller input.

**Keyword extraction:** The draft step already has access to `config.business.product_keywords` etc. We reuse these as the topic keywords for ancestor retrieval. This is semantically correct — we want ancestors that performed well on the same topics the user cares about.

### 4. Seed Pre-Compute Worker

#### `automation/seed_worker.rs` (~180 lines)

```rust
pub struct SeedWorker {
    db: DbPool,
    llm: Arc<dyn LlmProvider>,
    batch_size: u32,           // default 5
}

impl SeedWorker {
    pub fn new(db: DbPool, llm: Arc<dyn LlmProvider>) -> Self { ... }

    /// Run the seed worker loop until cancellation.
    ///
    /// On each tick:
    /// 1. Query up to batch_size content_nodes with status='pending'
    /// 2. For each node, generate 1-3 draft seeds via LLM
    /// 3. Store seeds in draft_seeds with default weight 0.5
    /// 4. Mark node as 'processed'
    ///
    /// The worker uses a long interval (5 minutes) and yields between
    /// batches to stay low-priority.
    pub async fn run(
        &self,
        cancel: CancellationToken,
        scheduler: LoopScheduler,
    ) { ... }

    /// Process a single content node, returning the number of seeds generated.
    async fn process_node(&self, node: &ContentNode) -> Result<u32, WorkflowError> { ... }
}
```

**LLM prompt for seed extraction:**
```
System: You are an expert at extracting tweetable hooks from written content.
Given a note/article, identify 1-3 distinct angles that could each become a
standalone tweet. For each, output a one-line hook (max 200 chars) and suggest
a tweet format (list, tip, question, contrarian_take, storytelling, before_after).

Format your response as:
HOOK: <hook text>
FORMAT: <format name>
---
```

**Parsing:** The response is split on `---` delimiters (same pattern as thread parsing). Each `HOOK:` + `FORMAT:` pair becomes a `draft_seed` row.

**Non-blocking design:**
- The worker runs on its own `LoopScheduler` with a 5-minute interval
- Processes at most `batch_size` (5) nodes per tick
- Uses `tokio::task::yield_now()` between nodes to avoid starving other tasks
- If LLM call fails for a node, it remains `pending` and will be retried next tick
- No channel coordination needed — the worker polls the DB directly

**Cold-start weight:** All seeds from LLM extraction start with `engagement_weight = 0.5` (the cold-start baseline from AD-5). As the user publishes tweets from these seeds and they accumulate performance data, the Winning DNA pipeline can adjust weights in future sessions.

### 5. Tests

#### `context/winning_dna_tests.rs` (via `#[cfg(test)] mod tests` in winning_dna.rs)

**Classification tests (deterministic, no DB):**
- `classify_reply_question` — text with "?" → `ask_question`
- `classify_reply_experience` — text with "I've noticed" → `share_experience`
- `classify_reply_data` — text with "research shows" → `add_data`
- `classify_reply_disagree` — text with "however, I think" → `respectful_disagree`
- `classify_reply_default` — generic agreement → `agree_and_expand`
- `classify_tweet_list` — numbered items → `list`
- `classify_tweet_contrarian` — "most people think" → `most_people_think_x`
- `classify_tweet_question` — ends with "?" → `question`
- `classify_tweet_tip` — actionable statement → `tip`
- `classify_tweet_default` → `storytelling`

**Scoring tests (deterministic, no DB):**
- `engagement_score_normalization` — score/max_score produces correct 0-1 range
- `engagement_score_zero_max` — max_score=0 returns 0.5 (cold-start)
- `retrieval_weight_no_decay` — days_since=0 → weight = engagement_score
- `retrieval_weight_half_life` — days_since=half_life → weight ≈ engagement_score/2
- `retrieval_weight_old_content` — days_since=56 (4x half_life) → very low weight

**Ranking tests (requires test DB):**
- `retrieve_ancestors_empty_db` — returns empty vec
- `retrieve_ancestors_ranks_by_weight` — higher engagement_score ranks first
- `retrieve_ancestors_recency_breaks_ties` — same engagement, recent wins
- `retrieve_ancestors_filters_low_engagement` — score < 0.1 excluded
- `cold_start_falls_back_to_seeds` — no performance data → returns content seeds
- `build_draft_context_formats_prompt` — output contains "Winning patterns" header
- `build_draft_context_caps_length` — prompt_block does not exceed 2000 chars

**Storage tests (new in analytics.rs and watchtower/mod.rs):**
- `update_and_get_tweet_archetype` — archetype_vibe persists
- `update_and_get_engagement_score` — engagement_score persists
- `get_max_performance_score_empty` — returns 0.0
- `get_max_performance_score_with_data` — returns correct max
- `get_scored_ancestors_matches_topic` — keyword filtering works
- `get_pending_content_nodes` — returns only pending nodes
- `mark_node_processed` — changes status, no longer in pending query
- `insert_seed_with_weight` — custom weight persists
- `get_seeds_for_context` — joins with content_nodes for title

---

## Risks and Mitigations

| Risk | Impact | Mitigation |
|------|--------|-----------|
| Rule-based classifiers produce poor archetype labels | Low — labels affect retrieval weighting, not LLM output quality. Worst case: uniform weighting (equivalent to random selection) | Make classifiers conservative: default to `agree_and_expand` / `storytelling`. Document as v1 heuristic, can upgrade to LLM-based in future session |
| `LIKE '%keyword%'` is slow on large reply_performance tables | Low — SQLite FTS would help but is premature for v1 scale. Most users will have <10K replies | Add index on `reply_performance(reply_id)` (already exists). If needed, S05+ can add FTS5 |
| Seed worker LLM calls add cost | Medium — 5 nodes × 1 LLM call each = 5 calls per 5-minute tick | Use same `max_tokens: 200` as reply generation. Worker is low-priority and processes small batches. Can be disabled by not configuring content sources |
| RAG context bloats system prompt past model context limit | Low — 2000-char cap ≈ 500 tokens. System prompt without RAG is ~300-500 tokens. Total stays well under 4K | Hard cap enforced in `build_draft_context()`. Truncation at content boundary (full ancestor or nothing) |
| Existing draft tests break | Medium — `generate_reply_with_archetype` still exists and is unchanged | New method `generate_reply_with_context` is additive. Only `workflow/draft.rs` changes its call site. Existing generator tests are unaffected |
| 500-line file limit exceeded | Medium — `analytics.rs` is already ~888 lines with tests | Analytics additions are ~60 lines of functions + ~40 lines of tests. File will be ~990 lines. If it exceeds 1000, extract the new ancestor-related queries into `storage/analytics/ancestors.rs` (module directory refactor). Plan for this contingency |

---

## Order of Operations

### Phase 1: Storage Layer (no behavioral changes)

**Step 1.1:** Add storage helpers to `storage/analytics.rs`
- `update_tweet_archetype()`, `update_reply_archetype()`
- `update_tweet_engagement_score()`, `update_reply_engagement_score()`
- `get_max_performance_score()`
- `AncestorRow` struct and `get_scored_ancestors()` query
- Tests for all new functions

**Step 1.2:** Add storage helpers to `storage/watchtower/mod.rs`
- `get_pending_content_nodes()`, `mark_node_processed()`
- `insert_draft_seed_with_weight()`, `SeedWithContext`, `get_seeds_for_context()`
- Tests for all new functions

**Checkpoint:** `cargo test -p tuitbot-core` passes. No behavioral changes yet.

### Phase 2: Classification & Retrieval (new module, no integration)

**Step 2.1:** Create `context/winning_dna.rs`
- `classify_reply_archetype()`, `classify_tweet_format()`
- `compute_engagement_score()`, `compute_retrieval_weight()`
- `WinningAncestor`, `ContentSeedContext`, `DraftContext` structs
- `retrieve_ancestors()`, `retrieve_cold_start_seeds()`, `build_draft_context()`
- All classification/scoring/ranking tests

**Step 2.2:** Update `context/mod.rs`
- Add `pub mod winning_dna;`

**Checkpoint:** `cargo test -p tuitbot-core` passes. Module exists but is not called from any production path.

### Phase 3: Content Generator Extension

**Step 3.1:** Add `generate_reply_with_context()` to `content/generator.rs`
- Method signature with optional `rag_context: Option<&str>` parameter
- System prompt assembly with optional RAG block
- Test with mock provider (RAG context appears in system prompt)

**Checkpoint:** `cargo test -p tuitbot-core` passes.

### Phase 4: Draft Pipeline Integration

**Step 4.1:** Modify `workflow/draft.rs` to call `build_draft_context` and `generate_reply_with_context`
- Extract topic keywords from config
- Call `build_draft_context()` before the candidate loop (one DB call, shared across all candidates)
- Pass `context.prompt_block` to each `generate_reply_with_context()` call
- Existing `DraftInput` unchanged — context sourced from DB

**Checkpoint:** `cargo test --workspace` passes. The draft path now injects RAG context, but gracefully degrades (empty context) when no ancestors/seeds exist.

### Phase 5: Seed Pre-Compute Worker

**Step 5.1:** Create `automation/seed_worker.rs`
- `SeedWorker` struct with `run()` and `process_node()` methods
- LLM prompt for hook extraction
- Response parsing
- Integration with storage helpers

**Step 5.2:** Update `automation/mod.rs`
- Add `pub mod seed_worker;`
- Add re-export for `SeedWorker`

**Checkpoint:** `cargo test --workspace` passes. Worker is defined but not spawned from server (that's for future integration).

### Phase 6: Documentation & Quality Gates

**Step 6.1:** Write `docs/roadmap/cold-start-watchtower-rag/rag-ranking.md`
- Scoring formula and normalization
- Retrieval algorithm (with recency decay)
- Classification heuristics and their accuracy expectations
- Threshold table with defaults and rationale
- Cold-start fallback behavior

**Step 6.2:** Write `docs/roadmap/cold-start-watchtower-rag/session-04-handoff.md`
- Summary of changes
- Files created/modified
- Decisions made
- Next session deliverables

**Step 6.3:** Run CI checklist
```bash
cargo fmt --all && cargo fmt --all --check
RUSTFLAGS="-D warnings" cargo test --workspace
cargo clippy --workspace -- -D warnings
```

---

## Thresholds Reference (for `rag-ranking.md`)

| Parameter | Default | Rationale |
|-----------|---------|-----------|
| `RECENCY_HALF_LIFE_DAYS` | 14.0 | Two weeks: recent successes matter more, but older hits still contribute. Decay to ~6% at 8 weeks |
| `MAX_ANCESTORS` | 5 | Keeps prompt additions to ~2500 chars. Diminishing returns beyond 5 examples |
| `COLD_START_WEIGHT` | 0.5 | Midpoint — unscored content gets equal chance. Neither favored nor penalized |
| `MIN_ENGAGEMENT_SCORE` | 0.1 | Excludes bottom ~10% performers. Prevents worst content from influencing new drafts |
| `RAG_MAX_CHARS` | 2000 | Conservative estimate at ~500 tokens. Keeps total system prompt well under 4K tokens |
| `SEED_BATCH_SIZE` | 5 | Process 5 content nodes per worker tick. Balances throughput vs. LLM cost |
| `SEED_WORKER_INTERVAL_SECS` | 300 | 5 minutes. Low priority — no urgency in seed generation |

All thresholds are defined as `const` values in `context/winning_dna.rs` and `automation/seed_worker.rs`. They are not configurable via TOML in v1 — YAGNI until user feedback indicates tuning is needed.

---

## Analytics.rs File Size Contingency

The `storage/analytics.rs` file is currently ~888 lines (including tests). Adding ~100 lines brings it to ~990. If during implementation it exceeds 1000 lines, the file should be refactored into a module directory:

```
storage/analytics/
  mod.rs          — re-exports, FollowerSnapshot, core types
  performance.rs  — reply_performance, tweet_performance CRUD
  scores.rs       — content_scores, summaries
  ancestors.rs    — NEW: get_scored_ancestors, archetype/engagement updates
  tests.rs        — all analytics tests
```

This follows the established pattern from `storage/approval_queue/` and `storage/watchtower/`. The decision to split will be made during implementation based on actual line count.

---

## Verification Steps

1. **Unit tests pass:** All new tests in `winning_dna`, `analytics`, `watchtower/mod`, and `generator` pass with `cargo test -p tuitbot-core`
2. **Integration test:** The `workflow/draft.rs` existing tests still pass (RAG context is empty in test DB → no behavioral change)
3. **Server tests:** `cargo test -p tuitbot-server` passes (no server changes in this session)
4. **Full workspace:** `RUSTFLAGS="-D warnings" cargo test --workspace`
5. **Clippy clean:** `cargo clippy --workspace -- -D warnings`
6. **Format clean:** `cargo fmt --all && cargo fmt --all --check`
7. **Manual verification:** Inspect that `build_draft_context` returns formatted text with ancestors when test DB has performance data
